@misc{horikawa2016,
      title={Generic decoding of seen and imagined objects using hierarchical visual features},
      author={Tomoyasu Horikawa and Yukiyasu Kamitani},
      year={2016},
      eprint={1510.06479},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}


@inproceedings{chenRethinkingVisualReconstruction2023,
	title = {Rethinking {Visual} {Reconstruction}: {Experience}-{Based} {Content} {Completion} {Guided} by {Visual} {Cues}},
	shorttitle = {Rethinking {Visual} {Reconstruction}},
	url = {https://proceedings.mlr.press/v202/chen23v.html},
	abstract = {Decoding seen images from brain activities has been an absorbing field. However, the reconstructed images still suffer from low quality with existing studies. This can be because our visual system is not like a camera that ”remembers” every pixel. Instead, only part of the information can be perceived with our selective attention, and the brain ”guesses” the rest to form what we think we see. Most existing approaches ignored the brain completion mechanism. In this work, we propose to reconstruct seen images with both the visual perception and the brain completion process, and design a simple, yet effective visual decoding framework to achieve this goal. Specifically, we first construct a shared discrete representation space for both brain signals and images. Then, a novel self-supervised token-to-token inpainting network is designed to implement visual content completion by building context and prior knowledge about the visual objects from the discrete latent space. Our approach improved the quality of visual reconstruction significantly and achieved state-of-the-art.},
	language = {en},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Jiaxuan and Qi, Yu and Pan, Gang},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {4856--4866},
	file = {Full Text PDF:/home/yiqian/yichian-gdrive/zotero-references/storage/H3KH2D2R/Chen et al. - 2023 - Rethinking Visual Reconstruction Experience-Based.pdf:application/pdf},
}

@misc{oordNeuralDiscreteRepresentation2018,
	title = {Neural {Discrete} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1711.00937},
	doi = {10.48550/arXiv.1711.00937},
	abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
	urldate = {2024-03-31},
	publisher = {arXiv},
	author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
	month = may,
	year = {2018},
	note = {arXiv:1711.00937 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/yiqian/yichian-gdrive/zotero-references/storage/6439QDPF/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/home/yiqian/yichian-gdrive/zotero-references/storage/J4XJ7XW3/1711.html:text/html},
}
